{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Neural Network\n",
    "The following code implements a neural network from scratch using only `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Activation:\n",
    "    def __init__(self, f, df):\n",
    "        \"\"\"\n",
    "        Initialize Activation object. Represents the activation fn for a layer\n",
    "        \n",
    "        :params f: function that takes column vector and returns column vector\n",
    "        :params df: corresponding derivative of f\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.f(x)\n",
    "    \n",
    "    def call_df(self ,x):\n",
    "        return self.df(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function\n",
    "\n",
    "    :params x: np vector\n",
    "    :returns: np vector\n",
    "    \"\"\"\n",
    "    x[x < 0] = 0\n",
    "\n",
    "    return x\n",
    "def d_relu(x):\n",
    "    \"\"\"\n",
    "    ReLU derivative\n",
    "    \n",
    "    :params x: np vector of shape mx1\n",
    "    :returns: np matrix of shape mxm\n",
    "    \"\"\"\n",
    "    x = relu(x)\n",
    "    x[x > 0] = 1\n",
    "    \n",
    "    return np.diag(x.T[0])\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "\n",
    "    \n",
    "    :params x: np vector\n",
    "    :returns: np vector\n",
    "    \"\"\"    \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid derivative\n",
    "    \n",
    "    :params x: np vector of shape mx1\n",
    "    :returns: np matrix of shape mxm\n",
    "    \"\"\"\n",
    "    exp = np.exp(-x)\n",
    "    return np.diag((exp/(1+exp)**2).T[0])\n",
    "\n",
    "def linear(x):\n",
    "    \"\"\"\n",
    "    Linear activation function\n",
    "    \n",
    "    :params x: np vector of shape mx1\n",
    "    :returns: np vector of shape mx1\n",
    "    \"\"\"\n",
    "    return x\n",
    "\n",
    "def d_linear(x):\n",
    "    \"\"\"\n",
    "    Linear derivative\n",
    "    \n",
    "    :params x: np vector of shape mx1\n",
    "    :returns: np matrix of shape mxm\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    return np.identity(m)\n",
    "\n",
    "activation_functions = {\n",
    "    \"relu\": Activation(relu, d_relu),\n",
    "    \"sigmoid\": Activation(sigmoid, d_sigmoid),\n",
    "    \"linear\": Activation(linear, d_linear)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, f, df):\n",
    "        \"\"\"\n",
    "        Initialize Loss object. Represents the loss function for a model\n",
    "        \n",
    "        :params f: fn taking ans cv and prediction cv and return loss (1, 1)\n",
    "        :params df: corresponding derivative of f w.r.t prediction\n",
    "        \"\"\"\n",
    "        self.f = f\n",
    "        self.df = df\n",
    "    \n",
    "    def call(self, g, a):\n",
    "        \"\"\"\n",
    "        :params g: guesses of model. (n, 1)\n",
    "        :params a: answers to guesses. (n, 1)\n",
    "        \"\"\"\n",
    "        return self.f(g, a)\n",
    "    \n",
    "    def call_df(self, g, a, x):\n",
    "        return self.df(g, a, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(g, a):\n",
    "    \"\"\"\n",
    "    Hinge loss\n",
    "    \n",
    "    :params g: column vector of guesses for all data pt\n",
    "    :params a: column vector of answers\n",
    "    \"\"\"\n",
    "    x = 1 - np.multiply(g, a)\n",
    "    x[x < 0] = 0\n",
    "    \n",
    "    return np.sum(x)\n",
    "\n",
    "def d_hinge_loss(g, a, x):\n",
    "    \"\"\"\n",
    "    Hinge loss derivative for single data point\n",
    "    \n",
    "    :params g: model guess\n",
    "    :params a: correct answer\n",
    "    :params x: column vector for data pt of shape mx1\n",
    "    :returns: column vector of shape mx1\n",
    "    \"\"\"\n",
    "    m, n = x.shape\n",
    "    if a*g > 1:\n",
    "        return np.zeros((m, 1))\n",
    "    else:\n",
    "        return -a*x\n",
    "\n",
    "def NLL(g, a):\n",
    "    \"\"\"\n",
    "    Negative least likelihood\n",
    "    \n",
    "    :params g: column vector of guesses for all data pt\n",
    "    :p\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "loss_functions = {\n",
    "    \"hinge\": Loss(hinge_loss, d_hinge_loss)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of a neural network layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, w, af, output=False):\n",
    "        \"\"\"\n",
    "        Initialize a Layer object.\n",
    "        Only can perform stochastic gradient descent\n",
    "        \n",
    "        :params w: mxn np matrix where m is input vec length, n is no. of units\n",
    "        :params af: activation function object\n",
    "        :params output: boolean for whether layer is output layer or intermediate\n",
    "        \"\"\"\n",
    "        m, n = w.shape\n",
    "        \n",
    "        self.weights = w\n",
    "        self.activation_fn = af\n",
    "        \n",
    "        self.input_dim = (m, 1)\n",
    "        self.output_dim = (n, 1)\n",
    "        \n",
    "        self.output_layer = output\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        run input vector through layer\n",
    "\n",
    "        :params x: column vector of size self.input_dim\n",
    "        :returns: column vector of size ouput_dim\n",
    "        \"\"\"\n",
    "        if x.shape != self.input_dim:\n",
    "            raise Exception(\"Wrong input dimension\")\n",
    "        return self.activation_fn.call(self.weights.T.dot(x))\n",
    "    \n",
    "    def backprop_call(self, x):\n",
    "        \"\"\"\n",
    "        run input vector through layer to produce outputs for backprop\n",
    "        \n",
    "        :params x: column vector of size self.input_dim\n",
    "        :returns: tuple (z, a), where z is tf by units, and a is activation\n",
    "        \"\"\"\n",
    "        if x.shape != self.input_dim:\n",
    "            raise Exception(\"Wrong input dimension\")\n",
    "        z = self.weights.T.dot(x)\n",
    "        a = self.activation_fn.call(z)\n",
    "        \n",
    "        return(z, a)\n",
    "    \n",
    "    def dadz(self, z):\n",
    "        \"\"\"\n",
    "        Get dadz for backprop. Refer to the math\n",
    "        \n",
    "        :params z: tf from this layer units\n",
    "        :returns: nxn matrix\n",
    "        \"\"\"\n",
    "        return self.activation_fn.call_df(z)\n",
    "        \n",
    "    \n",
    "    def dldz(self, dldz_front, w_front, z):\n",
    "        \"\"\"\n",
    "        get dldz for backprop. Refer to the math.\n",
    "        \n",
    "        :params dldz_front: column vector of dldz for layer in front (k, 1)\n",
    "        :params w_front: weights of layer in front (n, k)\n",
    "        :params z: output of current layer tf for the given data point (n, 1)\n",
    "        :returns: column vector of dldz (n, 1)\n",
    "        \"\"\"\n",
    "        if self.output: raise Exception(\"Output layer\")\n",
    "        return activation_fn.call_df(z).dot(w_front).dot(dldz_front)\n",
    "        \n",
    "    \n",
    "    def dldw(self, a, dldz):\n",
    "        \"\"\"\n",
    "        get gradient of weights for backprop. Refer to the math.\n",
    "        \n",
    "        :params dldz: (n, 1)\n",
    "        :params a: input from layer below (m, 1)\n",
    "        :returns: column vector of dldw (m, n)\n",
    "        \"\"\"\n",
    "        return a.dot(dldz.T)\n",
    "    \n",
    "    \n",
    "    def sgd_step(self, dldw, step_size):\n",
    "        \"\"\"\n",
    "        update weights according to given gradient and step size\n",
    "        \n",
    "        :params dldw: gradient of current weights w.r.t data pt (m, n)\n",
    "        :params step_size: step size to take. number\n",
    "        :returns: True\n",
    "        \"\"\"\n",
    "        self.weights = self.weights - dldw*step_size\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        create a neural network\n",
    "        \n",
    "        :params params: parameters of neural network\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_functions[params[\"loss\"]]\n",
    "        \n",
    "        def create_layer(input_dim, unit_no, af, output=False):\n",
    "            #no gain\n",
    "            w = np.random.normal(size=(input_dim, unit_no))\n",
    "            return Layer(w, af, output)\n",
    "        \n",
    "        self.layers = []\n",
    "        for idx, layer in enumerate(params[\"layers\"]):\n",
    "            unit_no, af = layer\n",
    "            layer_obj = None\n",
    "            if idx == 0:\n",
    "                layer_obj = create_layer(\\\n",
    "                                       params[\"input_dim\"],\\\n",
    "                                       unit_no,\\\n",
    "                                       activation_functions[af]\n",
    "                                      )\n",
    "            else:\n",
    "                unit_no_before, _ = params[\"layers\"][idx - 1]\n",
    "                layer_obj = create_layer(\\\n",
    "                                         unit_no_before,\\\n",
    "                                         unit_no,\\\n",
    "                                         activation_functions[af]\n",
    "                                         )\n",
    "            if idx == len(params[\"layers\"]) - 1:\n",
    "                layer_obj.output_layer = True\n",
    "                                         \n",
    "            self.layers.append(layer_obj)\n",
    "            \n",
    "    def run(self, X):\n",
    "        \"\"\"\n",
    "        Run data through the neural network\n",
    "        \n",
    "        :params X: mxn matrix where m is data dim and n is no of data pt\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        Y = []\n",
    "        #X = np.concatenate((X, np.ones((1, n))), axis=0)\n",
    "        for x in X.T:\n",
    "            x = np.array([x]).T\n",
    "            for layer in self.layers:\n",
    "                x = layer.call(x)\n",
    "            Y.append(x[0])\n",
    "        return np.array(Y)\n",
    "        \n",
    "    def train(self, data, labels, T=1000):\n",
    "        \"\"\"\n",
    "        Train model using stochastic gradient descent\n",
    "        \n",
    "        :params data: mxn matrix where m is data dim and n is no. of data pt\n",
    "        :params labels: 1xn matrix for the labels\n",
    "        \"\"\"\n",
    "        m, n = data.shape\n",
    "        d_loss = self.loss_fn.call_df\n",
    "        \n",
    "        step_fn = lambda t: 1/(t+1)\n",
    "        for t in range(T):\n",
    "            i = np.random.randint(n)\n",
    "            data_pt = np.array([data.T[i]]).T\n",
    "            label = np.array([labels[0,i]])\n",
    "            #print(label)\n",
    "            layer_outputs = [(None, data_pt)]\n",
    "            x = data_pt\n",
    "            #Forward pass to compute layer outputs\n",
    "            for layer in self.layers:\n",
    "                z, a = layer.backprop_call(x)\n",
    "                x = a\n",
    "                layer_outputs.append((z, a))\n",
    "            prev_dldz = None\n",
    "            next_weights = None\n",
    "            _, last_a = layer_outputs[-1]\n",
    "            pred = 1\n",
    "            if last_a <= 0:\n",
    "                pred = -1\n",
    "            #print(\"prediction:\", pred, \"actual:\", label)\n",
    "            x = data_pt\n",
    "            for idx in range(len(self.layers)-1, -1, -1):\n",
    "                z, a = layer_outputs[idx+1]\n",
    "                layer = self.layers[idx]\n",
    "                _, prev_a = layer_outputs[idx]\n",
    "                #prev_a = np.array([np.append(prev_a.T[0], 1)]).T\n",
    "                if layer.output_layer:\n",
    "                    print(\"guess: \",a[0,0],\", label: \", label[0],\",data pt: \", x)\n",
    "                    dlda = d_loss(a[0,0], label[0], x)\n",
    "                    dadz = layer.dadz(z)\n",
    "                    dldz = dadz.dot(dlda)\n",
    "                    dldw = prev_a.dot(dldz.T)\n",
    "                    print(\"dlda\", dlda, \"dadz\", dadz, \"dldw\", dldw)\n",
    "                    next_weights = np.copy(layer.weights)\n",
    "                    layer.sgd_step(dldw, step_fn(t))\n",
    "                    prev_dldz = dldz\n",
    "                else:\n",
    "                    dadz = layer.dadz(z)\n",
    "                    dldz = prev_dldz.dot(next_weights.T).dot(dadz)\n",
    "                    prev_dldz = dldz\n",
    "                    dldw = prev_a.dot(dldz)\n",
    "                    next_weights = np.copy(layer.weights)\n",
    "                    #print(\"old\", idx, next_weights)\n",
    "                    layer.sgd_step(dldw, step_fn(t))\n",
    "                    #print(\"new\", idx, layer.weights)\n",
    "                #print('old weights', next_weights)\n",
    "        print(\"Training complete\")\n",
    "\n",
    "                    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model\n",
    "Test model on dataset:\n",
    "```\n",
    "data: [-1, -1, 1, 1, ...]\n",
    "labels: [-1, -1, 1, 1, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(g, a):\n",
    "    \"\"\"\n",
    "    metric for how many guesses are correct\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    n, _ = g.shape\n",
    "    for i, x in enumerate(g):\n",
    "        if x[0] == a[0, i]:\n",
    "            score += 1\n",
    "    return score/n\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained average accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "example_params = {\n",
    "    \"loss\": \"hinge\",\n",
    "    \"layers\": [(2, \"relu\"),(2, \"relu\"),(1, \"linear\")],\n",
    "    \"input_dim\": 1\n",
    "}\n",
    "\n",
    "nn = NeuralNetwork(example_params)\n",
    "\n",
    "untrained_acc = 0\n",
    "for i in range(30):\n",
    "    labels = np.random.rand(1, 1000)\n",
    "    labels[labels > 0.5] = 1\n",
    "    labels[labels <= 0.5] = -1\n",
    "    g = nn.run(labels)\n",
    "    g[g > 0] = 1\n",
    "    g[g<=0] = -1\n",
    "    predictions = g\n",
    "    untrained_acc += accuracy(g, labels)\n",
    "print(\"Untrained average accuracy:\", untrained_acc/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guess:  0.14128915889491445 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[-1.]] dadz [[1.]] dldw [[-0.0106326 ]\n",
      " [-0.07929505]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  0.0 , label:  -1.0 ,data pt:  [[-1.]]\n",
      "dlda [[-1.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  0.0 , label:  -1.0 ,data pt:  [[-1.]]\n",
      "dlda [[-1.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  4.340532508598916 , label:  1.0 ,data pt:  [[1.]]\n",
      "dlda [[0.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "guess:  0.0 , label:  -1.0 ,data pt:  [[-1.]]\n",
      "dlda [[-1.]] dadz [[1.]] dldw [[0.]\n",
      " [0.]]\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "data = np.random.rand(1, 10000)\n",
    "data[data > 0.5] = 1\n",
    "data[data <= 0.5] = -1\n",
    "nn.train(data, data, T=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained average accuracy: 0.49000000000000005\n"
     ]
    }
   ],
   "source": [
    "trained_acc = 0\n",
    "for i in range(30):\n",
    "    labels = np.random.rand(1, 10)\n",
    "    labels[labels > 0.5] = 1\n",
    "    labels[labels <= 0.5] = -1\n",
    "    g = nn.run(labels)\n",
    "    g[g > 0.5] = 1\n",
    "    g[g<=0.5] = -1\n",
    "    predictions = g\n",
    "    trained_acc += accuracy(g, labels)\n",
    "print(\"Trained average accuracy:\", trained_acc/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
